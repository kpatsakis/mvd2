 perf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs) if ( current_is_64bit ( ) )  perf_callchain_user_32 ( entry , regs ); static void perf_callchain_user_32(struct perf_callchain_entry struct pt_regs *regs) unsigned int sp , next_sp ; unsigned int next_ip ; unsigned int lr ; long level = 0 ; unsigned int __user * fp , * uregs ; next_ip = perf_instruction_pointer ( regs ); lr = regs -> link; sp = regs -> gpr [ 1 ]; while ( entry -> nr < PERF_MAX_STACK_DEPTH )  fp = ( unsigned int __user * ) ( unsigned long ) sp; if ( ! valid_user_sp ( sp , 0 ) || read_user_stack_32 ( fp , & next_sp ) )  if ( level > 0 && read_user_stack_32 ( & fp [ 1 ] , & next_ip ) )  uregs = signal_frame_32_regs ( sp , next_sp , next_ip ); if ( ! uregs && level <= 1 )  uregs = signal_frame_32_regs ( sp , next_sp , lr ); if ( uregs )  if ( read_user_stack_32 ( & uregs [ PT_NIP ] , & next_ip ) || read_user_stack_32 ( & uregs [ PT_LNK ] , & lr ) || read_user_stack_32 ( & uregs [ PT_R1 ] , & sp ) )  level = 0; if ( level == 0 )  next_ip = lr; sp = next_sp; static int read_user_stack_32(unsigned int __user *ptr, unsigned int *ret) if ( ( unsigned long ) ptr > TASK_SIZE - sizeof ( unsigned int ) || ( ( unsigned long ) ptr & 3 ) )  if ( ! __get_user_inatomic ( * ret , ptr ) )  return read_user_stack_slow ( ptr , ret , 4 ) ; static int read_user_stack_slow(void __user *ptr, void *ret, int nb) pgd_t * pgdir ; pte_t * ptep , pte ; unsigned shift ; unsigned long addr = ( unsigned long ) ptr ; unsigned long offset ; unsigned long pfn ; void * kaddr ; pgdir = current -> mm -> pgd; if ( ! pgdir )  ptep = find_linux_pte_or_hugepte ( pgdir , addr , & shift ); if ( ! shift )  shift = PAGE_SHIFT; offset = addr & ( ( 1UL << shift ) - 1 ); if ( ptep == NULL )  pte = * ptep; if ( ! pte_present ( pte ) || ! ( pte_val ( pte ) & _PAGE_USER ) )  pfn = pte_pfn ( pte ); if ( ! page_is_ram ( pfn ) )  kaddr = pfn_to_kaddr ( pfn ); memcpy ( ret , kaddr + offset , nb ); 