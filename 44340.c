static sg_open(struct inode *inode, struct file *filp) int dev = iminor ( inode ) ; int flags = filp -> f_flags ; struct request_queue * q ; Sg_device * sdp ; Sg_fd * sfp ; int retval ; if ( ( flags & O_EXCL ) && ( O_RDONLY == ( flags & O_ACCMODE ) ) )  sdp = sg_get_dev ( dev ); static Sg_device sg_get_dev(int dev) struct sg_device * sdp ; sdp = sg_lookup_dev ( dev ); static Sg_device *sg_lookup_dev(int dev) return idr_find ( & sg_index_idr , dev ) ; if ( ! sdp )  sdp = ERR_PTR ( - ENXIO ); if ( atomic_read ( & sdp -> detaching ) )  sdp = ERR_PTR ( - ENODEV ); return sdp ; if ( IS_ERR ( sdp ) )  retval = scsi_device_get ( sdp -> device ); if ( retval )  retval = scsi_autopm_get_device ( sdp -> device ); if ( retval )  if ( ! ( ( flags & O_NONBLOCK ) || scsi_block_when_processing_errors ( sdp -> device ) ) )  if ( flags & O_NONBLOCK )  if ( flags & O_EXCL )  if ( sdp -> open_cnt > 0 )  if ( sdp -> exclude )  retval = open_wait ( sdp , flags ); static open_wait(Sg_device *sdp, int flags) int retval = 0 ; if ( flags & O_EXCL )  while ( sdp -> open_cnt > 0 )  retval = wait_event_interruptible ( sdp -> open_wait , ( atomic_read ( & sdp -> detaching ) || ! sdp -> open_cnt ) ); if ( retval )  return retval ; if ( atomic_read ( & sdp -> detaching ) )  return - ENODEV ; while ( sdp -> exclude )  retval = wait_event_interruptible ( sdp -> open_wait , ( atomic_read ( & sdp -> detaching ) || ! sdp -> exclude ) ); if ( retval )  return retval ; if ( atomic_read ( & sdp -> detaching ) )  return - ENODEV ; return retval ; if ( retval )  if ( flags & O_EXCL )  sdp -> exclude = true; if ( sdp -> open_cnt < 1 )  sdp -> sgdebug = 0; q = sdp -> device -> request_queue; sdp -> sg_tablesize = queue_max_segments ( q ); sfp = sg_add_sfp ( sdp ); static Sg_fd sg_add_sfp(Sg_device * sdp) Sg_fd * sfp ; int bufflen ; sfp = kzalloc ( sizeof ( * sfp ) , GFP_ATOMIC | __GFP_NOWARN ); if ( ! sfp )  sfp -> timeout = SG_DEFAULT_TIMEOUT; sfp -> timeout_user = SG_DEFAULT_TIMEOUT_USER; sfp -> force_packid = SG_DEF_FORCE_PACK_ID; sfp -> low_dma = ( SG_DEF_FORCE_LOW_DMA == 0 ) ? sdp -> device -> host -> unchecked_isa_dma : 1; sfp -> cmd_q = SG_DEF_COMMAND_Q; sfp -> keep_orphan = SG_DEF_KEEP_ORPHAN; sfp -> parentdp = sdp; if ( atomic_read ( & sdp -> detaching ) )  if ( unlikely ( sg_big_buff != def_reserved_size ) )  sg_big_buff = def_reserved_size; bufflen = min_t ( int , sg_big_buff , max_sectors_bytes ( sdp -> device -> request_queue ) ); static int max_sectors_bytes(struct request_queue *q) unsigned int max_sectors = queue_max_sectors ( q ) ; max_sectors = min_t ( unsigned int , max_sectors , INT_MAX >> 9 ) return max_sectors << 9 ; sg_build_reserve ( sfp , bufflen ); static sg_build_reserve(Sg_fd * sfp, int req_size) Sg_scatter_hold * schp = & sfp -> reserve ; if ( req_size < PAGE_SIZE )  req_size = PAGE_SIZE; if ( 0 == sg_build_indirect ( schp , sfp , req_size ) )  sg_remove_scat ( sfp , schp ); req_size >>= 1; while ( req_size > ( PAGE_SIZE / 2 ) )  static sg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp) memset ( schp , 0 , sizeof ( * schp ) ); 